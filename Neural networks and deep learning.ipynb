{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks and deep learning\n",
    "\n",
    "URL: http://neuralnetworksanddeeplearning.com/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Recognizing handwritten digits\n",
    "\n",
    "## Perceptrons\n",
    "\n",
    "* A type of artificial neuron\n",
    "* Takes in some binary inputs, and has a binary output\n",
    "* The output (0 or 1) is determined by the weighted sum of the inputs being greater than some threshold ($\\sum_j w_jx_j$ > thresold)\n",
    "\n",
    "E.g., there is a music concert. We think of 3 features:\n",
    "\n",
    "1. Do I like the music?\n",
    "2. Is it far?\n",
    "3. Is it expensive?\n",
    "\n",
    "We assign an important to all of these (weights), and then take a weighted sum. If it's over some value, I go to the concert.\n",
    "\n",
    "Now, we could update the definition of the peceptron a bit. If we moved the threshold to the other side of the equation and think of it instead as a bias, we something more familiar:\n",
    "\n",
    "$w\\cdot{x} + bias > 0$, then output 1 (where $w\\cdot{x}$ is our weighted sum).\n",
    "\n",
    "Conceptually, the bias can be thought of as how easy it is to get the perceptron to activate.\n",
    "\n",
    "## Sigmoid neurons\n",
    "\n",
    "Imagine we are designing a network to learn handwriting recognition, and we want to tweak our weights to improve it.\n",
    "Well, what we want is for a small change in any weight (or bias) to result in only a small change to our output.\n",
    "\n",
    "The problem with perceptrons is that a small change in any weight or bias can cause the output to flip from 0 to 1, and that can then cause another perceptron to flip from 0 to 1, etc. So even a small change can have a big effect.\n",
    "\n",
    "So we use another kind of neuron: __the sigmoid neuron__.\n",
    "Sigmoid neurons are like perceptrons, but a key property is that a small change to a weight or bias only results in a small change to the output.\n",
    "\n",
    "* Sigmoid neurons and cake in any value between 0-1\n",
    "* Their output is $\\sigma(w\\cdot{x}+b)$\n",
    "* The output will also be any number between 0-1\n",
    "\n",
    "$\\sigma$ is the function that squishes values into 0-1. A.k.a, the _logistic function_. Due to this alternate terminology, sigmoid neurons are sometimes called _logistic neurons_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUrmLiGSh/wMu/HNySa51rQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x28d7929eb38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The sigmoid function aka logistic function\n",
    "# Sigmoid will be close to 0 when -ve, and close to 1 when +ve\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    # Apply sigmoid activation function\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "test_input = np.arange(-6, 6, 0.01)\n",
    "plt.plot(test_input, sigmoid(test_input), linewidth=2)\n",
    "plt.grid(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the sigmoid function looks like a smoothed out binary step function, where anything > 0 becomes one. So we can kinda think of a sigmoid neuron as a \"smoothed out\" perceptron.\n",
    "\n",
    "_Note: Sigmoid is a common activation function, but there are others._\n",
    "\n",
    "\n",
    "## The architecture of neural networks\n",
    "\n",
    "* __Feedforward neural networks__: The output from one neuron becomes the input to another. Information always go forward -- there is no feedback/loops.\n",
    "\n",
    "* Input layer\n",
    "* Output layer\n",
    "* Hidden layer (any layer which is not an input or output layer)\n",
    "\n",
    "\n",
    "## A simple network to classify handwritten digits\n",
    "\n",
    "To classify a string of digits, we must do 2 things:\n",
    "\n",
    "1. Split the string into individual digits (_segmentation_)\n",
    "2. Classify each digit\n",
    "\n",
    "We'll focus on 2.\n",
    "\n",
    "The network will be structured as follows:\n",
    "\n",
    "1. Input layer: 784 neurons, one for each pixel in a 28*28 pixel image. Each input is the grayscale value of a neuron, where 0.0 = white and 1.0 = black\n",
    "2. A hidden layer containing _n_ neurons\n",
    "3. A 10 neuron output layer. If the output of the first neuron is ~1, then the network thinks the input digit is a 1, etc. I.e., we check for the highest-valued neuron in this layer to see which digit has been recognized.\n",
    "\n",
    "<img src=\"images/neural_net_deep_learning/net1.jpg\" width=\"450\">\n",
    "\n",
    "\n",
    "## Learning with gradient descent\n",
    "\n",
    "Use $x$ to denote a training input (i.e., grayscale image of a character).\n",
    "Each training input can be thought of as a 28x28 = 784 dimensional vector. Each entry in the vector represents the grayscale value for a pixel in the image.\n",
    "\n",
    "We'll denote the output as $y=y(x)$ where y is a 10 dimensional vector.\n",
    "For example, if training image $x$ denotes a 6, then $y(x) = (0, 0, 0, 0, 0, 0, 1, 0, 0, 0)^T$. (Note the $^T$ just denotes transforming the row vector into a column vector.)\n",
    "\n",
    "Now, we want to find all the weights and biases that for any given input will give us our desired output.\n",
    "\n",
    "To measure how well we're doing, we'll define a __cost function__ (aka _loss_ or _objective_ function).\n",
    "\n",
    "* Cost function: A function that measures how accurate your function is\n",
    "\n",
    "Here is our cost function:\n",
    "\n",
    "$C(w,b) = \\frac{1}{2n} \\displaystyle\\sum_x \\parallel{y(x)-a}\\parallel^2$\n",
    "\n",
    "* $w$ = all weights\n",
    "* $b$ = all biases\n",
    "* $x$ = input\n",
    "* $n$ = number of training inputs\n",
    "* $a$ = vector of outputs (since this is the output, it depends on $x$, $y$, and $b$\n",
    "\n",
    "$\\parallel{v}\\parallel$ denotes the length of a vector.\n",
    "\n",
    "We'll call C the _quadratic cost function_, aka, _Mean Squared Error_ or just _MSE_ function.\n",
    "\n",
    "Things to note:\n",
    "\n",
    "* The value will always be positive since we $^2$ the difference\n",
    "* The value will be close to 0 when $y(x)$ and $a$ are almost equal\n",
    "* I.e., the closer the function is to 0, the better our network is working\n",
    "\n",
    "So, we want to find a set of weights and biases that make the cost function as low as possible.\n",
    "\n",
    "The algorithm for finding the weights and biases that make the cost function as low as possible is known as __gradient descent__.\n",
    "\n",
    "For now, forget about the network, sigmoid, etc., etc., and just focus on how we can minimize a function using gradient descent.\n",
    "\n",
    "Let's just imagine a surface defined by 2 variables. We want to move down the surface until we reach the bottom, and then we know which two variables _minimize_ the function.\n",
    "\n",
    "<img src=\"images/neural_net_deep_learning/multivarfunc.jpg\" width=\"450\">\n",
    "\n",
    "In the diagram above, what happens when we move the ball some small amount $\\Delta{v1}$ in the $v1$ direction, and some small amount $\\Delta{v2}$ in the $v2$ direction? According to calculus:\n",
    "\n",
    "$\\Delta{C} = \\frac{\\delta{C}}{\\delta{v_1}}\\Delta{v_1} + \\frac{\\delta{C}}{\\delta{v_2}}\\Delta{v_2}$\n",
    "\n",
    "\n",
    "The change in C = the change in C with respect to V1 * the change in V1 + the change in C with respect to V2 * the change in V2\n",
    "\n",
    "We want to choose $\\Delta{v_1}$ and $\\Delta{v_2}$ so as to make C negative -- to move down our slope/minimize our cost function.\n",
    "\n",
    "(Hint: Recall that the gradient of a function -- all of the partial derivatives stuffed into a vector -- gives you the slope of steepest ascent.)\n",
    "\n",
    "Let's define:\n",
    "\n",
    "$\\Delta{v} = (\\Delta{v_1}, \\Delta{v_2})^T$ = the column vector of changes to the $v$ variables\n",
    "\n",
    "$\\nabla{C} = (\\frac{\\delta{C}}{\\delta{v_1}}, \\frac{\\delta{C}}{\\delta{v_2}})^T$ = gradient of C = the column vector of partial derivatives\n",
    "\n",
    "Recall $\\nabla$ tells you something is a gradient vector -- a vector of partial derivatives.\n",
    "\n",
    "Anyway, this let's us rewrite:\n",
    "\n",
    "$\\Delta{C} = \\nabla{C}\\cdot{\\Delta{v}}$\n",
    "\n",
    "Or in English: the change in C is given by the gradient of C * the change in directions.\n",
    "\n",
    "So, to move \"down\" the function -- the reduce our cost -- we just need to choose $v$ such that:\n",
    "\n",
    "$v \\to v' = v-\\eta{\\nabla{C}}$, where $\\eta$ (eta) is some small, positive parameter (known as the __learning rate__).\n",
    "\n",
    "In other words, we multiply the gradient by some negative step/learning rate to move down a certain amount, and take that away from our current position (input variables -- our weights and biases) to move down.\n",
    "\n",
    "In other other words, we repeatedly compute the gradient of C, then move down in the _opposite_ direction until we reach the bottom of our function.\n",
    "\n",
    "Note that this will still work even if we have more than 2 variables in $v$.\n",
    "\n",
    "We must be careful to choose a learning rate (step) that is not too big (could overshoot 0 and end up +ve), or too small (could be too slow and we'd never reach 0).\n",
    "\n",
    "__To sum up__:\n",
    "Gradient descent is taking small steps in the direction which does the most to immediately decrease C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gradient descent in neural networks\n",
    "\n",
    "The idea is to use gradient descent to find the weights $w_k$ and biases $b_l$ that will minimize our cost function:\n",
    "\n",
    "$C(w,b) = \\frac{1}{2n} \\displaystyle\\sum_x \\parallel{y(x)-a}\\parallel^2$\n",
    "\n",
    "In other words, our position $v$ will be replaced by the weights and biases. Restating the rule for gradient descent in these terms, we get:\n",
    "\n",
    "$w_k \\to w'_k = w_k - \\eta\\frac{\\delta{C}}{\\partial{w_k}}$\n",
    "\n",
    "$b_l \\to b'_l = b_l - \\eta\\frac{\\delta{C}}{\\partial{b_l}}$\n",
    "\n",
    "### Problem\n",
    "Take another look at the cost function:\n",
    "\n",
    "$C(w,b) = \\frac{1}{2n} \\displaystyle\\sum_x \\parallel{y(x)-a}\\parallel^2$\n",
    "\n",
    "It is an average of the differences for all the training inputs: you add up all the differences for every training input, and then divide them by 1/2n.\n",
    "\n",
    "This means we need to computer gradients for each training input and then average them, which might take a long time when there is so much training data.\n",
    "\n",
    "### Solution\n",
    "__Stochastic gradient descent__ can speed up the learning.\n",
    "\n",
    "The idea: Estimate the gradient $\\nabla{C}$ by computing it for a small sample of training inputs. By averaging for this small sample, we can quickly get a good estimate of the true gradient, which speeds things up.\n",
    "\n",
    "The small sample we pick is called a __mini-batch__, and has size $m$.\n",
    "\n",
    "So our formula for the gradient of this mini-batch becomes:\n",
    "\n",
    "$\\nabla{C} = \\frac{1}{m}\\displaystyle\\sum_{j=1}^{m} \\nabla{C_{X_j}}$\n",
    "\n",
    "Specifically for our neural network:\n",
    "\n",
    "$w_k \\to w'_k = w_k - \\frac{\\eta}{m}\\displaystyle\\sum_{j}\\frac{\\delta{C_{X_j}}}{\\delta{d_{w_k}}}$\n",
    "\n",
    "$b_l \\to b'_l = b_l - \\frac{\\eta}{m}\\displaystyle\\sum_{j}\\frac{\\delta{C_{X_j}}}{\\delta{d_{b_l}}}$\n",
    "\n",
    "Reminder: You are multiplying the gradient by $\\eta$, so $\\frac{1}{m}$ becomes $\\frac{\\eta}{m}$.\n",
    "\n",
    "Really the above formulas are just saying, to calculate the weights:\n",
    "\n",
    "1. Take the derivative (the gradient) of our cost function with respect to $w$ for each sample input. This makes sense -- we're saying, how does $C$ change with respect to $w$?\n",
    "2. Sum up the values for all of those sample input gradients\n",
    "3. Multiply them by $\\frac{\\eta}{m}$ -- in other words, take the average, then multiply by our step size/learning rate $\\eta$\n",
    "4. Take this value away from our current weight value to move down the function\n",
    "\n",
    "We pick out a mini-batch, train with it, and repeat until we've exhaused the training inputs. We then say we have completed an __epoch__ of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Implementing the network to classify digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The centerpiece is this Network class:\n",
    "import numpy as np\n",
    "\n",
    "class Network():\n",
    "    \n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        # The number of neurons in each layer\n",
    "        # E.g., Network([2, 3, 1]) would initialize 2 neurons in the first layer, \n",
    "        # 3 in the second, and 1 in the final layer\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[-1.25628833],\n",
      "       [-1.7767921 ],\n",
      "       [ 0.39810403]]), array([[ 0.20693542]])]\n"
     ]
    }
   ],
   "source": [
    "# Example: Initialize a network with 2 inputs, 3 neurons in the hidden layer\n",
    "#          and 1 neuron in the output layer\n",
    "net = Network([2, 3, 1])\n",
    "\n",
    "print(net.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.25628833]\n",
      " [-1.7767921 ]\n",
      " [ 0.39810403]]\n"
     ]
    }
   ],
   "source": [
    "# biases for hidden layer\n",
    "print(net.biases[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.20693542]]\n"
     ]
    }
   ],
   "source": [
    "# biases for output layer\n",
    "print(net.biases[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 1.18408177,  0.93486811],\n",
      "       [ 0.58617594, -0.69009554],\n",
      "       [ 0.20493135,  0.8747818 ]]), array([[ 0.65670335, -0.15479206,  1.2972806 ]])]\n"
     ]
    }
   ],
   "source": [
    "print(net.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.18408177  0.93486811]\n",
      " [ 0.58617594 -0.69009554]\n",
      " [ 0.20493135  0.8747818 ]]\n"
     ]
    }
   ],
   "source": [
    "# weights from intput to hidden layer\n",
    "# (2 input neurons * 3 hidden layer neurons = 6)\n",
    "print(net.weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.65670335 -0.15479206  1.2972806 ]]\n"
     ]
    }
   ],
   "source": [
    "# weights from hidden layer to output\n",
    "print(net.weights[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will call the `net.weights[1]` matrix $w$ such that $w_{jk}$ is the weight for the connection between the $k^{th}$ neuron in the second layer and the $j^{th}$ neuron in the third layer.\n",
    "\n",
    "This means that the activations of the third layer is given by:\n",
    "\n",
    "$a' = \\sigma(wa + b)$\n",
    "\n",
    "* $a'$ = vector of activation values for the third layer\n",
    "* $w$ = weight matrix between the second and third layers\n",
    "* $a$ = vector of activation values for the second layer\n",
    "* $b$ = vector of biases\n",
    "\n",
    "So we multiply our second layer activations by the weight matrix, and add on our biases, and then run the the result through our sigma function. This gives us the third layer activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's define the sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now let's add the feedfoward method and stochastic gradient descent method to Network\n",
    "# Given an input a, it will return output using the formula above\n",
    "class Network():\n",
    "    \n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        # The number of neurons in each layer\n",
    "        # E.g., Network([2, 3, 1]) would initialize 2 neurons in the first layer, \n",
    "        # 3 in the second, and 1 in the final layer\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "    # New!\n",
    "    # Given previous activation, calculate next activation\n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "    \n",
    "    # New!\n",
    "    # Stochastic gradient descent\n",
    "    def stochastic_gradient_descent(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The \"training_data\" is a list of tuples\n",
    "        \"(x, y)\" representing the training inputs and the desired\n",
    "        outputs.  \n",
    "        \n",
    "        If \"test_data\" is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        \n",
    "        if test_data:\n",
    "            n_test = len(test_data)\n",
    "            \n",
    "        n = len(training_data)\n",
    "        \n",
    "        for j in xrange(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            \n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in xrange(0, n, mini_batch_size)\n",
    "            ]\n",
    "            \n",
    "            for mini_batch in mini_batches:\n",
    "                self-update_mini_batch(mini_batch, eta)\n",
    "                \n",
    "            if test_data:\n",
    "                print(\"Epoch {0}: {1}/{2}\".format(j, self.evaluate(test_data), n_test))\n",
    "            else:\n",
    "                print(\"Epoch {0} complete\".format(j))\n",
    "                \n",
    "    # New!\n",
    "    # Applies a single step of gradient descent,\n",
    "    # updating network weights and biases.\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The \"mini_batch\" is a list of tuples \"(x, y)\", and \"eta\"\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x, y in mini_batch:\n",
    "            # Calculate gradient for biases and weights\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            # Add this gradient to current biases gradient?\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            # Add this gradient to current weight gradient?\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            \n",
    "        # Update weights -- move down using our gradient and learning rate\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        \n",
    "        # Likewise for biases\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters:\n",
    "\n",
    "* `training_data` = list of tuples `(x, y)` representing training inputs and desired outputs\n",
    "* `epochs` = the epoches to train for (number of training data sets)\n",
    "* `mini_batch_size` = size of mini-batches...obviously...\n",
    "* `eta` = learning rate (step size)($\\eta$)\n",
    "* `test_data` = if supplied, will print out partial progress after each epoch\n",
    "\n",
    "### Flow:\n",
    "\n",
    "For each epoch:\n",
    "\n",
    "1. Randomly shuffle the training data\n",
    "2. Split it into mini_batches\n",
    "3. For each mini_batch:\n",
    "4. Apply a single step of gradient descent (`update_mini_batch`) (This updates network weights and biases)\n",
    "\n",
    "`update_mini_batch` uses `backprop` (will be shown later) to compute gradients for every training sample in `mini_batch` and hen updates `weights` and `biases` using the functions we derived earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete listing:\n",
    "# %load network.py\n",
    "\n",
    "\"\"\"\n",
    "network.py\n",
    "~~~~~~~~~~\n",
    "IT WORKS\n",
    "A module to implement the stochastic gradient descent learning\n",
    "algorithm for a feedforward neural network.  Gradients are calculated\n",
    "using backpropagation.  Note that I have focused on making the code\n",
    "simple, easily readable, and easily modifiable.  It is not optimized,\n",
    "and omits many desirable features.\n",
    "\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import random\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "\n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)\n",
    "\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            if test_data:\n",
    "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data),n_test));\n",
    "            else:\n",
    "                print(\"Epoch {} complete\".format(j))\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * \\\n",
    "            sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "\n",
    "#### Miscellaneous functions\n",
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Wrapper to help load data\n",
    "\n",
    "# %load mnist_loader.py\n",
    "\"\"\"\n",
    "mnist_loader\n",
    "~~~~~~~~~~~~\n",
    "A library to load the MNIST image data.  For details of the data\n",
    "structures that are returned, see the doc strings for ``load_data``\n",
    "and ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the\n",
    "function usually called by our neural network code.\n",
    "\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# Third-party libraries\n",
    "import numpy as np\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Return the MNIST data as a tuple containing the training data,\n",
    "    the validation data, and the test data.\n",
    "    The ``training_data`` is returned as a tuple with two entries.\n",
    "    The first entry contains the actual training images.  This is a\n",
    "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
    "    numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
    "    pixels in a single MNIST image.\n",
    "    The second entry in the ``training_data`` tuple is a numpy ndarray\n",
    "    containing 50,000 entries.  Those entries are just the digit\n",
    "    values (0...9) for the corresponding images contained in the first\n",
    "    entry of the tuple.\n",
    "    The ``validation_data`` and ``test_data`` are similar, except\n",
    "    each contains only 10,000 images.\n",
    "    This is a nice data format, but for use in neural networks it's\n",
    "    helpful to modify the format of the ``training_data`` a little.\n",
    "    That's done in the wrapper function ``load_data_wrapper()``, see\n",
    "    below.\n",
    "    \"\"\"\n",
    "    f = gzip.open('mnist/mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    \"\"\"Return a tuple containing ``(training_data, validation_data,\n",
    "    test_data)``. Based on ``load_data``, but the format is more\n",
    "    convenient for use in our implementation of neural networks.\n",
    "    In particular, ``training_data`` is a list containing 50,000\n",
    "    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n",
    "    containing the input image.  ``y`` is a 10-dimensional\n",
    "    numpy.ndarray representing the unit vector corresponding to the\n",
    "    correct digit for ``x``.\n",
    "    ``validation_data`` and ``test_data`` are lists containing 10,000\n",
    "    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n",
    "    numpy.ndarry containing the input image, and ``y`` is the\n",
    "    corresponding classification, i.e., the digit values (integers)\n",
    "    corresponding to ``x``.\n",
    "    Obviously, this means we're using slightly different formats for\n",
    "    the training data and the validation / test data.  These formats\n",
    "    turn out to be the most convenient for use in our neural network\n",
    "    code.\"\"\"\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up a neural network\n",
    "net = Network([784, 100, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete\n",
      "Epoch 1 complete\n",
      "Epoch 2 complete\n",
      "Epoch 3 complete\n",
      "Epoch 4 complete\n",
      "Epoch 5 complete\n",
      "Epoch 6 complete\n",
      "Epoch 7 complete\n",
      "Epoch 8 complete\n",
      "Epoch 9 complete\n",
      "Epoch 10 complete\n",
      "Epoch 11 complete\n",
      "Epoch 12 complete\n",
      "Epoch 13 complete\n",
      "Epoch 14 complete\n",
      "Epoch 15 complete\n",
      "Epoch 16 complete\n",
      "Epoch 17 complete\n",
      "Epoch 18 complete\n",
      "Epoch 19 complete\n",
      "Epoch 20 complete\n",
      "Epoch 21 complete\n",
      "Epoch 22 complete\n",
      "Epoch 23 complete\n",
      "Epoch 24 complete\n",
      "Epoch 25 complete\n",
      "Epoch 26 complete\n",
      "Epoch 27 complete\n",
      "Epoch 28 complete\n",
      "Epoch 29 complete\n"
     ]
    }
   ],
   "source": [
    "# Run over 30 epochs, mini_batch_size = 10, learning rate (eta) = 3\n",
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Chapter 2: Backpropagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural nets learn their weights and biases using the gradient descent algorithm: calculating the gradient of the cost function, and then moving down.\n",
    "\n",
    "But how to do we computer the gradient of the cost function? Answer: backpropagation.\n",
    "\n",
    "__Backpropagation__: The algorithm for computing the gradient of a cost function\n",
    "\n",
    "At the heart of the backpropagation algorithm is the partial derivative:\n",
    "\n",
    "$C = \\frac{\\delta{C}}{\\delta{w}}$, how does the cost change with respect to the weights (or biases)\n",
    "\n",
    "First, some notation:\n",
    "\n",
    "For weights:\n",
    "\n",
    "<img src=\"images/neural_net_deep_learning/backpropnot.jpg\" width=\"450\">\n",
    "\n",
    "* $l$ = layer\n",
    "* $j$ = output neuron\n",
    "* $k$ = input neuron (neuron in layer before)\n",
    "\n",
    "For activations/biases:\n",
    "\n",
    "$b^l_j$ = $j^{th}$ neuron in the $l^{th}$ layer (using $a$ for activations)\n",
    "\n",
    "<img src=\"images/neural_net_deep_learning/backpropnot2.jpg\" width=\"450\">\n",
    "\n",
    "_(Skipping some of the working notes to save time crunching through formula notation)_\n",
    "\n",
    "The activation for a given neurons is given by the weighted sum of the activations*weights that connect from this neuron to the previous layer + bias for this neuron.\n",
    "\n",
    "Formally:\n",
    "\n",
    "$a^l_j = \\sigma(\\displaystyle\\sum_k{w^l_{jk}a^{l-1}_k + b^l_j})$\n",
    "\n",
    "Now, in our program we will be using matrices to store all the weights, so rewrite in matrix form we define a weight matrix:\n",
    "\n",
    "$w^l$ = the weights connecting to the $l^{th}$ layer of neurons.\n",
    "\n",
    "The entry in the $j^{th}$ row and $k^{th}$ column is $w^l_{jk}$\n",
    "\n",
    "Likewise we have a bias vector:\n",
    "\n",
    "$b^l$ = stores the bias values for each neuron in the layer l\n",
    "\n",
    "And finally an activation vector:\n",
    "\n",
    "$a^l$ = the activations in layer l\n",
    "\n",
    "This lets us rewrite our activation formula in the following way:\n",
    "\n",
    "$a^l = \\sigma({w^la^{l-1} + b^l})$\n",
    "\n",
    "Notice we have lost the sum: we just apply the weight matrix to the activation matrix, and then add the bias vector. Then run the whole lot through the sigmoid function.\n",
    "\n",
    "The expression in the middle we will call:\n",
    "\n",
    "$z^l = {w^la^{l-1} + b^l}$ == the _weighted input_ to the neurons in layer L.\n",
    "\n",
    "In other words, the activation value before it's run through the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of back propagation is to computer the partial derivatives $\\delta{C}/\\delta{w}$ and $\\delta{C}/\\delta{b}$ for any weight or bias in the network.\n",
    "\n",
    "Let's use the cost function from the last chapter:\n",
    "\n",
    "$C = \\frac{1}{2n} \\displaystyle\\sum_x \\parallel{y(x)-a^L(x)}\\parallel^2$\n",
    "\n",
    "* $n$ = number of training examples\n",
    "* $x$ = training example\n",
    "* $L$ = number of layers in the network\n",
    "* $a^L(x)$ = the vector of activations output from the network when $x$ is input\n",
    "\n",
    "In other words, we take the square sum of our expected values - activations for a bunch of samples, and multiply by 1/2*num of training examples.\n",
    "\n",
    "### Assumption 1\n",
    "The cost function can be written as an average. Since each run gives us the cost function for one example, assume we can sum up all of the costs and divide by the number of examples to get an average.\n",
    "\n",
    "### Assumption 2\n",
    "The cost function can written as a function of outputs from the neural network. In other words, it will include some $a^L$ term, which ours does, where $a^L$ is the vector of output activations.\n",
    "\n",
    "Note: y, the expected output, is a fixed value based on the input. It is not affected by weights or biases. Therefore, we can consider the cost function a function of the output activations $a^L$ only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Haramard/Schur product\n",
    "Multiplying two vectors elementwise:\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "1 \n",
    "\\\\2 \n",
    "\\end{bmatrix}$\n",
    "$\\odot$\n",
    "$\\begin{bmatrix}\n",
    "3\n",
    "\\\\4\n",
    "\\end{bmatrix}$\n",
    "=\n",
    "$\\begin{bmatrix}\n",
    "3\n",
    "\\\\8\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The four fundamental equations behind backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To computer our partial derivatives for weights and biases, we introduce an intermediate quantity:\n",
    "\n",
    "$\\delta^l_j$ = the __error__ in the $l^{th}$ layer, neuron $j$.\n",
    "\n",
    "Backpropagation will let us compute this error and then relate it to our weight/bias partial derivatives.\n",
    "\n",
    "Let's define it as:\n",
    "\n",
    "$\\delta^l_j = \\frac{\\delta{C}}{\\delta{z^l_j}}$\n",
    "\n",
    "Reminder: $z^l = {w^la^{l-1} + b^l}$ == the _weighted input_ to the neurons in layer L.\n",
    "\n",
    "\n",
    "-- So our error of a neuron j in layer l is defined as the derivative of C wth respect to the weighted for that neuron. In other words, how much the weighted input changes C?\n",
    "\n",
    "The \"demon\" is going to change this weighted input to help us and minimize our cost function.\n",
    "\n",
    "Note: We use the weighted input instead of activation in our calculations just to be a little bit simpler algebraically speaking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Plan of Attack\n",
    "Backpropagation is based around 4 fundamental equations. These equations will let us computer the error ($\\delta^l$) and the gradient of the cost function.\n",
    "\n",
    "### 1. Equation for the error in the output layer\n",
    "\n",
    "$\\delta^L_j = \\frac{\\delta{C}}{\\delta{a^L_j}} \\sigma'{(z^L_j)}$\n",
    "\n",
    "The error in the output layer is given:\n",
    "\n",
    "How fast the cost changes with relation to a given neuron in that layer $(\\frac{\\delta{C}}{\\delta{a^L_j}})$\n",
    "*\n",
    "The weighted input for a given neuron in that layer $z^l = {w^la^{l-1} + b^l}$, run through the derivative of the $\\sigma$ function.\n",
    "\n",
    "For the $(\\frac{\\delta{C}}{\\delta{a^L_j}})$ term: if C doesn't depend much on a particular neuron, then the error will be small.\n",
    "\n",
    "For the $\\sigma'{(z^l_j)}$ term: measures how fast the activation function is changing at the Z neuron (since sigma is the derivative).\n",
    "\n",
    "So conceptually, you have -- how much C depends on a particular output neuron * how fast the activation function is changing.\n",
    "\n",
    "In matrix form:\n",
    "\n",
    "$\\delta^L = \\nabla_a{C}\\odot{\\sigma'{(z^L)}}$\n",
    "\n",
    "Reminder: $\\nabla_a{C}$ = a vector whose components are the partial derivatives $\\delta{C}/\\delta{a^L_j}$.\n",
    "\n",
    "You can think of this vector as expressing the rate of change for C with respect to output activations.\n",
    "\n",
    "Now, keep in the quadratic cost function we have:\n",
    "\n",
    "$\\nabla_a{C} = (a^L - y)$\n",
    "\n",
    "So the fully matrix-based form of this equation becomes:\n",
    "\n",
    "$\\delta^L = (a^L - y)\\odot{\\sigma'(z^L)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Equation for the error in terms of the error in the next layer\n",
    "\n",
    "$d^l = ((w^{l+1})^T\\delta^{l+a})\\odot\\sigma'(z^l)$\n",
    "\n",
    "The transpose of the weight matrix in the next layer * the error for that layer, then do an elementwise multiplication with the weighted sum of this layer run through the derivative of the sigmoid function.\n",
    "\n",
    "Suppose we know the error at the $l+1$ layer. When we apply the ranspose weight matrix, we can think of it as moving the error backward through the network. Then taking the Hadamard product of the weighted input, this moves the error backward through the activation function in $l$, giving the error in the weighted input to layer $l$. (?)\n",
    "\n",
    "By combining this first and second equation, we can computer $\\delta^l$ for any layer in the network. The idea is, we compute the error at some layer $L$ usng the first equation, then use this equation to compute it for the layer $L-1$ and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3. Equation for rate of change of cost with respect to a particular bias\n",
    "\n",
    "$\\frac{\\delta{C}}{\\delta{b^l_l}} = \\delta^l_{j}$\n",
    "\n",
    "I.e., how the cost changes as a particular bias changes = the error for a particular node in a particular layer.\n",
    "\n",
    "We can rewrite this as:\n",
    "\n",
    "$\\frac{\\delta{C}}{\\delta{b}} = \\delta$\n",
    "\n",
    "Where it's understood that $\\delta$ and $b$ are being evaluated at the same neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Equation for rate of change of cost with respect to a particular weight\n",
    "\n",
    "$\\frac{\\delta{C}}{\\delta{w^l_{jk}}} = a^{l-1}_kd^l_j$\n",
    "\n",
    "I.e., how the cost changes as a particular weight changes = activation for the neuron connecting to this weight in the previous layer * the error for this neuron.\n",
    "\n",
    "Rewriting without the indices:\n",
    "\n",
    "$\\frac{\\delta{C}}{\\delta{w}} = a_{in}\\delta_{out}$\n",
    "\n",
    "a is the activation of the neuron input to the weight, and d is the error of the neuron output from teh weight.\n",
    "\n",
    "When $a_{in}$ is small, the derivative term will also be small.\n",
    "\n",
    "So we can say the weight _learns slowly_ -- it doesn't change much during gradient descent. In other words, weights output from low-activation neurons learn slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights\n",
    "Looking at various components of the equations, it's concluded that a weight will learn slowly if:\n",
    "\n",
    "1. The input neuron is low activation\n",
    "2. The output neuron has __saturated__ (has high/low activation) (derived from the way the sigmoid function will flatten out near 0-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of equations\n",
    "\n",
    "$\\delta^L = \\nabla_a{C}\\odot{\\sigma'{(z^L)}}$                (BP1)\n",
    "\n",
    "\n",
    "\n",
    "$d^l = ((w^{l+1})^T\\delta^{l+a})\\odot\\sigma'(z^l)$           (BP2) \n",
    "\n",
    "\n",
    "\n",
    "$\\frac{\\delta{C}}{\\delta{b^l_l}} = \\delta^l_{j}$             (BP3)\n",
    "\n",
    "\n",
    "\n",
    "$\\frac{\\delta{C}}{\\delta{w^l_{jk}}} = a^{l-1}_kd^l_j$            (BP4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The backpropagation algorithm\n",
    "The backpropagation equations that we found let us compute the gradient of the cost function. Let's rewrite as an algorithm:\n",
    "\n",
    "1. __Input x__: Set the activation $a^1$ for the input layer\n",
    "2. __Feedforward__: For each layer starting from the first hidden layer, $l = 2, 3, ...,L$, compute $z^l = w^la^{l-1} + b^l$, and $a^l = \\sigma{z^l}$ (I.e., compute the weighted sum and run it through the sigmoid function)\n",
    "3. __Output error__: Compute $d^L = \\nabla_aC\\odot\\sigma'(z^L)$\n",
    "4. __Backpropagate__: For each layer starting at the output-1 layer and working backwards $l = L-1, L-2,...,2$ compute $\\delta^l = ((w^{l+1})^T\\delta^{l+1}\\odot{\\sigma'(z^l)}$\n",
    "5. __Output__: The gradient of the cost function, given by the rate of change for the weight and biases:\n",
    "$\\frac{\\delta{C}}{\\delta{w^l_{jk}}} = a^{l-1}_kd^l_j$  \n",
    "and\n",
    "$\\frac{\\delta{C}}{\\delta{b^l_l}} = \\delta^l_{j}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
