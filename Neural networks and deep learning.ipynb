{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks and deep learning\n",
    "\n",
    "URL: http://neuralnetworksanddeeplearning.com/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Recognizing handwritten digits\n",
    "\n",
    "## Perceptrons\n",
    "\n",
    "* A type of artificial neuron\n",
    "* Takes in some binary inputs, and has a binary output\n",
    "* The output (0 or 1) is determined by the weighted sum of the inputs being greater than some threshold ($\\sum_j w_jx_j$ > thresold)\n",
    "\n",
    "E.g., there is a music concert. We think of 3 features:\n",
    "\n",
    "1. Do I like the music?\n",
    "2. Is it far?\n",
    "3. Is it expensive?\n",
    "\n",
    "We assign an important to all of these (weights), and then take a weighted sum. If it's over some value, I go to the concert.\n",
    "\n",
    "Now, we could update the definition of the peceptron a bit. If we moved the threshold to the other side of the equation and think of it instead as a bias, we something more familiar:\n",
    "\n",
    "$w\\cdot{x} + bias > 0$, then output 1 (where $w\\cdot{x}$ is our weighted sum).\n",
    "\n",
    "Conceptually, the bias can be thought of as how easy it is to get the perceptron to activate.\n",
    "\n",
    "## Sigmoid neurons\n",
    "\n",
    "Imagine we are designing a network to learn handwriting recognition, and we want to tweak our weights to improve it.\n",
    "Well, what we want is for a small change in any weight (or bias) to result in only a small change to our output.\n",
    "\n",
    "The problem with perceptrons is that a small change in any weight or bias can cause the output to flip from 0 to 1, and that can then cause another perceptron to flip from 0 to 1, etc. So even a small change can have a big effect.\n",
    "\n",
    "So we use another kind of neuron: __the sigmoid neuron__.\n",
    "Sigmoid neurons are like perceptrons, but a key property is that a small change to a weight or bias only results in a small change to the output.\n",
    "\n",
    "* Sigmoid neurons and cake in any value between 0-1\n",
    "* Their output is $\\sigma(w\\cdot{x}+b)$\n",
    "* The output will also be any number between 0-1\n",
    "\n",
    "$\\sigma$ is the function that squishes values into 0-1. A.k.a, the _logistic function_. Due to this alternate terminology, sigmoid neurons are sometimes called _logistic neurons_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUrmLiGSh/wMu/HNySa51rQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c44fe79f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The sigmoid function aka logistic function\n",
    "# Sigmoid will be close to 0 when -ve, and close to 1 when +ve\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    # Apply sigmoid activation function\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "test_input = np.arange(-6, 6, 0.01)\n",
    "plt.plot(test_input, sigmoid(test_input), linewidth=2)\n",
    "plt.grid(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the sigmoid function looks like a smoothed out binary step function, where anything > 0 becomes one. So we can kinda think of a sigmoid neuron as a \"smoothed out\" perceptron.\n",
    "\n",
    "_Note: Sigmoid is a common activation function, but there are others._\n",
    "\n",
    "\n",
    "## The architecture of neural networks\n",
    "\n",
    "* __Feedforward neural networks__: The output from one neuron becomes the input to another. Information always go forward -- there is no feedback/loops.\n",
    "\n",
    "* Input layer\n",
    "* Output layer\n",
    "* Hidden layer (any layer which is not an input or output layer)\n",
    "\n",
    "\n",
    "## A simple network to classify handwritten digits\n",
    "\n",
    "To classify a string of digits, we must do 2 things:\n",
    "\n",
    "1. Split the string into individual digits (_segmentation_)\n",
    "2. Classify each digit\n",
    "\n",
    "We'll focus on 2.\n",
    "\n",
    "The network will be structured as follows:\n",
    "\n",
    "1. Input layer: 784 neurons, one for each pixel in a 28*28 pixel image. Each input is the grayscale value of a neuron, where 0.0 = white and 1.0 = black\n",
    "2. A hidden layer containing _n_ neurons\n",
    "3. A 10 neuron output layer. If the output of the first neuron is ~1, then the network thinks the input digit is a 1, etc. I.e., we check for the highest-valued neuron in this layer to see which digit has been recognized.\n",
    "\n",
    "<img src=\"images/neural_net_deep_learning/net1.jpg\" width=\"450\">\n",
    "\n",
    "\n",
    "## Learning with gradient descent\n",
    "\n",
    "Use $x$ to denote a training input (i.e., grayscale image of a character).\n",
    "Each training input can be thought of as a 28x28 = 784 dimensional vector. Each entry in the vector represents the grayscale value for a pixel in the image.\n",
    "\n",
    "We'll denote the output as $y=y(x)$ where y is a 10 dimensional vector.\n",
    "For example, if training image $x$ denotes a 6, then $y(x) = (0, 0, 0, 0, 0, 0, 1, 0, 0, 0)^T$. (Note the $^T$ just denotes transforming the row vector into a column vector.)\n",
    "\n",
    "Now, we want to find all the weights and biases that for any given input will give us our desired output.\n",
    "\n",
    "To measure how well we're doing, we'll define a __cost function__ (aka _loss_ or _objective_ function).\n",
    "\n",
    "* Cost function: A function that measures how accurate your function is\n",
    "\n",
    "Here is our cost function:\n",
    "\n",
    "$C(w,b) = \\frac{1}{2n} \\displaystyle\\sum_x \\parallel{y(x)-a}\\parallel^2$\n",
    "\n",
    "* $w$ = all weights\n",
    "* $b$ = all biases\n",
    "* $x$ = input\n",
    "* $n$ = number of training inputs\n",
    "* $a$ = vector of outputs (since this is the output, it depends on $x$, $y$, and $b$\n",
    "\n",
    "$\\parallel{v}\\parallel$ denotes the length of a vector.\n",
    "\n",
    "We'll call C the _quadratic cost function_, aka, _Mean Squared Error_ or just _MSE_ function.\n",
    "\n",
    "Things to note:\n",
    "\n",
    "* The value will always be positive since we $^2$ the difference\n",
    "* The value will be close to 0 when $y(x)$ and $a$ are almost equal\n",
    "* I.e., the closer the function is to 0, the better our network is working\n",
    "\n",
    "So, we want to find a set of weights and biases that make the cost function as low as possible.\n",
    "\n",
    "The algorithm for finding the weights and biases that make the cost function as low as possible is known as __gradient descent__.\n",
    "\n",
    "For now, forget about the network, sigmoid, etc., etc., and just focus on how we can minimize a function using gradient descent.\n",
    "\n",
    "Let's just imagine a surface defined by 2 variables. We want to move down the surface until we reach the bottom, and then we know which two variables _minimize_ the function.\n",
    "\n",
    "<img src=\"images/neural_net_deep_learning/multivarfunc.jpg\" width=\"450\">\n",
    "\n",
    "In the diagram above, what happens when we move the ball some small amount $\\Delta{v1}$ in the $v1$ direction, and some small amount $\\Delta{v2}$ in the $v2$ direction? According to calculus:\n",
    "\n",
    "$\\Delta{C} = \\frac{\\delta{C}}{\\delta{v_1}}\\Delta{v_1} + \\frac{\\delta{C}}{\\delta{v_2}}\\Delta{v_2}$\n",
    "\n",
    "\n",
    "The change in C = the change in C with respect to V1 * the change in V1 + the change in C with respect to V2 * the change in V2\n",
    "\n",
    "We want to choose $\\Delta{v_1}$ and $\\Delta{v_2}$ so as to make C negative -- to move down our slope/minimize our cost function.\n",
    "\n",
    "(Hint: Recall that the gradient of a function -- all of the partial derivatives stuffed into a vector -- gives you the slope of steepest ascent.)\n",
    "\n",
    "Let's define:\n",
    "\n",
    "$\\Delta{v} = (\\Delta{v_1}, \\Delta{v_2})^T$ = the column vector of changes to the $v$ variables\n",
    "\n",
    "$\\nabla{C} = (\\frac{\\delta{C}}{\\delta{v_1}}, \\frac{\\delta{C}}{\\delta{v_2}})^T$ = gradient of C = the column vector of partial derivatives\n",
    "\n",
    "Recall $\\nabla$ tells you something is a gradient vector -- a vector of partial derivatives.\n",
    "\n",
    "Anyway, this let's us rewrite:\n",
    "\n",
    "$\\Delta{C} = \\nabla{C}\\cdot{\\Delta{v}}$\n",
    "\n",
    "Or in English: the change in C is given by the gradient of C * the change in directions.\n",
    "\n",
    "So, to move \"down\" the function -- the reduce our cost -- we just need to choose $v$ such that:\n",
    "\n",
    "$v \\to v' = v-\\eta{\\nabla{C}}$, where $\\eta$ (eta) is some small, positive parameter (known as the __learning rate__).\n",
    "\n",
    "In other words, we multiply the gradient by some negative step/learning rate to move down a certain amount, and take that away from our current position (input variables -- our weights and biases) to move down.\n",
    "\n",
    "In other other words, we repeatedly compute the gradient of C, then move down in the _opposite_ direction until we reach the bottom of our function.\n",
    "\n",
    "Note that this will still work even if we have more than 2 variables in $v$.\n",
    "\n",
    "We must be careful to choose a learning rate (step) that is not too big (could overshoot 0 and end up +ve), or too small (could be too slow and we'd never reach 0).\n",
    "\n",
    "__To sum up__:\n",
    "Gradient descent is taking small steps in the direction which does the most to immediately decrease C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gradient descent in neural networks\n",
    "\n",
    "The idea is to use gradient descent to find the weights $w_k$ and biases $b_l$ that will minimize our cost function:\n",
    "\n",
    "$C(w,b) = \\frac{1}{2n} \\displaystyle\\sum_x \\parallel{y(x)-a}\\parallel^2$\n",
    "\n",
    "In other words, our position $v$ will be replaced by the weights and biases. Restating the rule for gradient descent in these terms, we get:\n",
    "\n",
    "$w_k \\to w'_k = w_k - \\eta\\frac{\\delta{C}}{\\partial{w_k}}$\n",
    "\n",
    "$b_l \\to b'_l = b_l - \\eta\\frac{\\delta{C}}{\\partial{b_l}}$\n",
    "\n",
    "### Problem\n",
    "Take another look at the cost function:\n",
    "\n",
    "$C(w,b) = \\frac{1}{2n} \\displaystyle\\sum_x \\parallel{y(x)-a}\\parallel^2$\n",
    "\n",
    "It is an average of the differences for all the training inputs: you add up all the differences for every training input, and then divide them by 1/2n.\n",
    "\n",
    "This means we need to computer gradients for each training input and then average them, which might take a long time when there is so much training data.\n",
    "\n",
    "### Solution\n",
    "__Stochastic gradient descent__ can speed up the learning.\n",
    "\n",
    "The idea: Estimate the gradient $\\nabla{C}$ by computing it for a small sample of training inputs. By averaging for this small sample, we can quickly get a good estimate of the true gradient, which speeds things up.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
